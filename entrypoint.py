import os
import json

import pandas as pd
import sqlalchemy as sa
import google.oauth2.service_account
import googleapiclient.discovery
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import polyline
from shapely.geometry import Point, Polygon


# –°–µ–∫—Ä–µ—Ç—ã MySQL


def get_mysql_url() -> str:
    url = os.environ["mysql_url"]
    return url


def get_postgres_url() -> str:
    url = os.environ["postgres_url"]
    return url


def get_google_creds() -> str:
    url = os.environ["google_service_account_json"]
    return url


def read_sheet_data_to_pandas(service, spreadsheet_id: str, range_name: str):
    """
    –ß–∏—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Google –¢–∞–±–ª–∏—Ü—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ Pandas DataFrame.

    Args:
        service: –û–±—ä–µ–∫—Ç —Å–ª—É–∂–±—ã Google Sheets API.
        spreadsheet_id: ID Google –¢–∞–±–ª–∏—Ü—ã.
        range_name: –î–∏–∞–ø–∞–∑–æ–Ω —è—á–µ–µ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Sheet1!A1:D10').

    Returns:
        Pandas DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    if not service:
        return None

    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Sheets API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
        result = service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=range_name,
            majorDimension='ROWS' # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        ).execute()

        values = result.get('values', [])

        if not values:
            print(f"–í –¥–∏–∞–ø–∞–∑–æ–Ω–µ '{range_name}' —Ç–∞–±–ª–∏—Ü—ã '{spreadsheet_id}' –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
            return pd.DataFrame() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame

        # Pandas –º–æ–∂–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å–æ–∑–¥–∞—Ç—å DataFrame –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤.
        # –û–±—ã—á–Ω–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏.
        headers = values[0]
        data_rows = values[1:]

        # –°–æ–∑–¥–∞–µ–º Pandas DataFrame
        if headers:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame, –∏—Å–ø–æ–ª—å–∑—É—è –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            df = pd.DataFrame(data_rows, columns=headers)
        else:
            # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –¥–∞–Ω–Ω—ã—Ö
            df = pd.DataFrame(values)
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –°—Ç–æ–ª–±—Ü—ã –Ω–∞–∑–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (0, 1, 2...).")

        print(f"–î–∞–Ω–Ω—ã–µ –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ '{range_name}' —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ Pandas DataFrame.")
        return df

    except googleapiclient.errors.HttpError as error:
        print(f"–û—à–∏–±–∫–∞ Google Sheets API: {error}")
        print(f"–ö–æ–¥ –æ—à–∏–±–∫–∏: {error.resp.status}")
        print(f"–°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ: {error._get_reason()}")
        return None
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return None


def get_sheets_service(service_account_file: str):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å–ª—É–∂–±—ã Google Sheets API.

    Args:
        service_account_file: –ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞.

    Returns:
        –û–±—ä–µ–∫—Ç googleapiclient.discovery.Resource –¥–ª—è Sheets API.
    """
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±–ª–∞—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–∞. –î–ª—è —á—Ç–µ–Ω–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 'spreadsheets.readonly'.
        SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
        creds = google.oauth2.service_account.Credentials.from_service_account_file(
            service_account_file, scopes=SCOPES
        )

        # –°—Ç—Ä–æ–∏–º —Å–µ—Ä–≤–∏—Å Sheets API
        service = googleapiclient.discovery.build('sheets', 'v4', credentials=creds)
        print("–°–µ—Ä–≤–∏—Å Google Sheets API —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return service
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ Google Sheets API: {e}")
        return None

class GoogleSheetsManager:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Google Sheets —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –æ—á–∏—Å—Ç–∫–∏ –∏ –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö
    """

    def __init__(self, credentials_file, spreadsheet_id):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ Google Sheets

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        credentials_file (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å credentials —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
        spreadsheet_id (str): ID Google —Ç–∞–±–ª–∏—Ü—ã
        """
        self.spreadsheet_id = spreadsheet_id
        self.credentials_file = credentials_file
        self.service = self._authenticate()

    def _authenticate(self):
        """
        –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞ Google Sheets
        """
        try:
            credentials = google.oauth2.service_account.Credentials.from_service_account_file(
                self.credentials_file,
                scopes=['https://www.googleapis.com/auth/spreadsheets']
            )
            service = build('sheets', 'v4', credentials=credentials)
            return service
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            raise

    def get_sheet_metadata(self, sheet_name=None):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞, –≤–∫–ª—é—á–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Å—Ç–æ–ª–±—Ü–æ–≤

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        sheet_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        dict: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            spreadsheet = self.service.spreadsheets().get(
                spreadsheetId=self.spreadsheet_id,
                ranges=[],
                includeGridData=False
            ).execute()

            # –ù–∞—Ö–æ–¥–∏–º –Ω—É–∂–Ω—ã–π –ª–∏—Å—Ç
            if sheet_name:
                sheet = next((s for s in spreadsheet['sheets']
                              if s['properties']['title'] == sheet_name), None)
                if not sheet:
                    raise ValueError(f"–õ–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            else:
                sheet = spreadsheet['sheets'][0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç

            return {
                'sheet_id': sheet['properties']['sheetId'],
                'title': sheet['properties']['title'],
                'row_count': sheet['properties'].get('gridProperties', {}).get('rowCount', 1000),
                'column_count': sheet['properties'].get('gridProperties', {}).get('columnCount', 26)
            }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

    def truncate_sheet(self, range_name=None):
        """
        –û—á–∏—â–∞–µ—Ç –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (–∞–Ω–∞–ª–æ–≥ TRUNCATE)

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        range_name (str): –î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Sheet1!A1:Z1000')
                          –ï—Å–ª–∏ None, –æ—á–∏—â–∞–µ—Ç –≤–µ—Å—å –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            if range_name is None:
                # –ï—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –æ—á–∏—â–∞–µ–º –≤–µ—Å—å –ª–∏—Å—Ç
                sheet_meta = self.get_sheet_metadata()
                if sheet_meta:
                    # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –≤—Å–µ–≥–æ –ª–∏—Å—Ç–∞
                    last_column = self._get_column_letter(sheet_meta['column_count'])
                    range_name = f"{sheet_meta['title']}!A1:{last_column}{sheet_meta['row_count']}"
                else:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
                    range_name = 'Sheet1!A1:Z1000'

            # –û—á–∏—Å—Ç–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ clear
            result = self.service.spreadsheets().values().clear(
                spreadsheetId=self.spreadsheet_id,
                range=range_name
            ).execute()

            print(f"‚úÖ –î–∏–∞–ø–∞–∑–æ–Ω {range_name} —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω")
            return True

        except HttpError as error:
            print(f"‚ùå –û—à–∏–±–∫–∞ API –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {error}")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}")
            return False

    def truncate_sheet_batch(self, sheet_name=None):
        """
        –û—á–∏—â–∞–µ—Ç –ª–∏—Å—Ç —Å –ø–æ–º–æ—â—å—é batchUpdate (–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ª–∏—Å—Ç–æ–≤)

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        sheet_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞
            sheet_meta = self.get_sheet_metadata(sheet_name)

            if not sheet_meta:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞")
                return False

            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—á–∏—Å—Ç–∫—É —á–µ—Ä–µ–∑ repeatCell
            requests = [{
                "repeatCell": {
                    "range": {
                        "sheetId": sheet_meta['sheet_id'],
                        "startRowIndex": 0,
                        "endRowIndex": sheet_meta['row_count'],
                        "startColumnIndex": 0,
                        "endColumnIndex": sheet_meta['column_count']
                    },
                    "cell": {
                        "userEnteredValue": None  # –û—á–∏—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                    },
                    "fields": "userEnteredValue"
                }
            }]

            body = {
                'requests': requests
            }

            # –í—ã–ø–æ–ª–Ω—è–µ–º batchUpdate
            result = self.service.spreadsheets().batchUpdate(
                spreadsheetId=self.spreadsheet_id,
                body=body
            ).execute()

            print(f"‚úÖ –õ–∏—Å—Ç '{sheet_meta['title']}' —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω (batchUpdate)")
            return True

        except HttpError as error:
            print(f"‚ùå –û—à–∏–±–∫–∞ API –ø—Ä–∏ batch –æ—á–∏—Å—Ç–∫–µ: {error}")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ batch –æ—á–∏—Å—Ç–∫–µ: {e}")
            return False

    def _get_column_letter(self, column_number):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ –≤ –±—É–∫–≤–µ–Ω–Ω–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ (1 -> A, 26 -> Z, 27 -> AA)

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        column_number (int): –ù–æ–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞—á–∏–Ω–∞—è —Å 1)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        str: –ë—É–∫–≤–µ–Ω–Ω–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        """
        result = ""
        while column_number > 0:
            column_number -= 1
            result = chr(column_number % 26 + 65) + result
            column_number //= 26
        return result

    def write_dataframe(self, df, range_name, value_input_option='RAW', include_headers=True):
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç DataFrame –≤ Google Sheets

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df (pandas.DataFrame): DataFrame –¥–ª—è –∑–∞–ø–∏—Å–∏
        range_name (str): –î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∑–∞–ø–∏—Å–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Sheet1!A1')
        value_input_option (str): 'RAW' –∏–ª–∏ 'USER_ENTERED'
        include_headers (bool): –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            if include_headers:
                values = [df.columns.values.tolist()] + df.values.tolist()
            else:
                values = df.values.tolist()

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
            body = {
                'values': values
            }

            # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö
            result = self.service.spreadsheets().values().update(
                spreadsheetId=self.spreadsheet_id,
                range=range_name,
                valueInputOption=value_input_option,
                body=body
            ).execute()

            print(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ {result.get('updatedCells')} —è—á–µ–µ–∫")
            print(f"üìä –î–∏–∞–ø–∞–∑–æ–Ω: {result.get('updatedRange')}")
            print(f"üìù –ó–∞–≥–æ–ª–æ–≤–∫–∏: {'–≤–∫–ª—é—á–µ–Ω—ã' if include_headers else '–Ω–µ –≤–∫–ª—é—á–µ–Ω—ã'}")

            return result

        except HttpError as error:
            print(f"‚ùå –û—à–∏–±–∫–∞ API –ø—Ä–∏ –∑–∞–ø–∏—Å–∏: {error}")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏: {e}")
            return None

    def truncate_and_write(self, df, range_name, sheet_name=None,
                           use_batch_clear=False, include_headers=True):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É (TRUNCATE) –∏ –∑–∞–ø–∏—Å—å –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df (pandas.DataFrame): DataFrame –¥–ª—è –∑–∞–ø–∏—Å–∏
        range_name (str): –î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∑–∞–ø–∏—Å–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Sheet1!A1')
        sheet_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        use_batch_clear (bool): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batchUpdate –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        include_headers (bool): –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        print("üöÄ –ù–∞—á–∞–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–∏ truncate and write...")
        print("=" * 50)

        # –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("üìù –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")

        if use_batch_clear:
            success = self.truncate_sheet_batch(sheet_name)
        else:
            # –û—á–∏—â–∞–µ–º —Ç–æ—Ç –∂–µ –¥–∏–∞–ø–∞–∑–æ–Ω, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ–º –ø–∏—Å–∞—Ç—å
            success = self.truncate_sheet(range_name)

        if not success:
            print("‚ùå –û—á–∏—Å—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å. –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞.")
            return False

        print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        print("-" * 50)

        # –®–∞–≥ 2: –ó–∞–ø–∏—Å—å –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üìù –®–∞–≥ 2: –ó–∞–ø–∏—Å—å –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        result = self.write_dataframe(
            df=df,
            range_name=range_name,
            include_headers=include_headers
        )

        if result:
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã")
            print("=" * 50)
            print("üéâ –û–ø–µ—Ä–∞—Ü–∏—è truncate and write –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        else:
            print("‚ùå –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–∞—Å—å")
            return False

    def truncate_and_write_with_resize(self, df, sheet_name=None, start_cell='A1',
                                       include_headers=True):
        """
        –û—á–∏—â–∞–µ—Ç –ª–∏—Å—Ç –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–≥–æ–Ω—è—è —Ä–∞–∑–º–µ—Ä—ã

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df (pandas.DataFrame): DataFrame –¥–ª—è –∑–∞–ø–∏—Å–∏
        sheet_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞
        start_cell (str): –ù–∞—á–∞–ª—å–Ω–∞—è —è—á–µ–π–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
        include_headers (bool): –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            print("üöÄ –ù–∞—á–∞–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–∏ truncate, resize and write...")
            print("=" * 50)

            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞
            sheet_meta = self.get_sheet_metadata(sheet_name)

            if not sheet_meta:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–∞")
                return False

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            required_rows = len(df) + (1 if include_headers else 0)
            required_cols = len(df.columns)

            print(f"üìä –¢—Ä–µ–±—É–µ—Ç—Å—è —Å—Ç—Ä–æ–∫: {required_rows}, —Å—Ç–æ–ª–±—Ü–æ–≤: {required_cols}")

            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –æ—á–∏—Å—Ç–∫—É
            requests = [
                # –û—á–∏—â–∞–µ–º –≤—Å–µ —è—á–µ–π–∫–∏
                {
                    "repeatCell": {
                        "range": {
                            "sheetId": sheet_meta['sheet_id'],
                            "startRowIndex": 0,
                            "endRowIndex": sheet_meta['row_count'],
                            "startColumnIndex": 0,
                            "endColumnIndex": sheet_meta['column_count']
                        },
                        "cell": {
                            "userEnteredValue": None
                        },
                        "fields": "userEnteredValue"
                    }
                },
                # –ò–∑–º–µ–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                {
                    "updateSheetProperties": {
                        "properties": {
                            "sheetId": sheet_meta['sheet_id'],
                            "gridProperties": {
                                "rowCount": max(required_rows, sheet_meta['row_count']),
                                "columnCount": max(required_cols, sheet_meta['column_count'])
                            }
                        },
                        "fields": "gridProperties.rowCount,gridProperties.columnCount"
                    }
                }
            ]

            # –í—ã–ø–æ–ª–Ω—è–µ–º batchUpdate
            body = {'requests': requests}
            self.service.spreadsheets().batchUpdate(
                spreadsheetId=self.spreadsheet_id,
                body=body
            ).execute()

            print("‚úÖ –õ–∏—Å—Ç –æ—á–∏—â–µ–Ω –∏ —Ä–∞–∑–º–µ—Ä—ã —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã")

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            range_name = f"{sheet_meta['title']}!{start_cell}"
            result = self.write_dataframe(
                df=df,
                range_name=range_name,
                include_headers=include_headers
            )

            if result:
                print("=" * 50)
                print("üéâ –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                return True
            else:
                return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False

def decode_polyline_to_tuples(encoded_polyline_string):
    coordinates_tuples = polyline.decode(encoded_polyline_string)
    return coordinates_tuples

def poly_contains(df):
    a = df['area_detail_tuple']
    b = df['parking_detail_tuple']
    res = Polygon(a).contains(Polygon(b))
    return res

def poly_contains_point_kvt(df):
    start_point = Point(df['g_lat'], df['g_lng'])
    area_poly = Polygon(df['area_poly'])
    res = area_poly.contains(start_point)
    return res

def poly_contains_point_orders(df):
    start_point = Point(df['start_lat'], df['start_lng'])
    area_poly = Polygon(df['area_poly'])
    res = area_poly.contains(start_point)
    return res

def main():

    url = get_mysql_url()
    url = sa.engine.make_url(url)
    url = url.set(drivername="mysql+mysqlconnector")
    engine_mysql = sa.create_engine(url)

    url = get_postgres_url()
    url = sa.engine.make_url(url)
    url = url.set(drivername="postgresql+psycopg")
    engine_postgresql = sa.create_engine(url)

    google_service_account_json = get_google_creds()

    with open('google_json.json', 'w') as fp:
        json.dump(json.loads(google_service_account_json, strict=False), fp)
    generated_json_file = './google_json.json'


    # –í—ã–≥—Ä—É–∑–∫–∞ t_area_revenue_stats2 pandas. –ù–∞—á–∞–ª–æ



    select_kvt = '''
        SELECT 
            res_tab."timestamp" ,
            res_tab.timestamp_hour ,
            res_tab.id ,
            res_tab.city_id ,
            res_tab.g_lat ,
            res_tab.g_lng
        FROM 
        (
            SELECT 
                tbh."timestamp" ,
                date_trunc('hour', tbh."timestamp") AS "timestamp_hour" ,
                tbh.id ,
                tbh.g_lat ,
                tbh.g_lng ,
                tb.city_id ,
                RANK() OVER (PARTITION BY date_trunc('hour', tbh."timestamp") ORDER BY tbh."timestamp" DESC) AS rn
            FROM damir.t_bike_history tbh
            LEFT JOIN damir.t_bike tb ON tbh.id = tb.id 
            WHERE tbh.error_status IN (0,7)
                AND tbh."timestamp" >= date_trunc('hour', NOW())
            ) AS res_tab
        WHERE res_tab.rn = 1
    '''
    df_kvt = pd.read_sql(select_kvt, engine_postgresql)

    # –í—ã–≥—Ä—É–∑–∫–∞ areas
    select_areas = '''
        SELECT
            --ta.city_id ,
            ta.id AS area_id ,
            ta."name" AS area_name ,
            ta.detail AS area_detail
        FROM damir.t_area ta
        WHERE ta."name" LIKE '%%| Area |%%'
    '''
    df_areas = pd.read_sql(select_areas, engine_postgresql)
    # area –≤ –ø–æ–ª–∏–≥–æ–Ω—ã
    df_areas['area_poly'] = df_areas['area_detail'].apply(decode_polyline_to_tuples)

    df_kvt_area = df_kvt.merge(df_areas, how='cross')
    df_kvt_area['res'] = df_kvt_area.apply(poly_contains_point_kvt, axis=1)
    df_kvt_area_res = df_kvt_area[df_kvt_area['res'] == True]
    df_kvt_area_res = df_kvt_area_res.drop(
        columns=['timestamp', 'city_id', 'g_lat', 'g_lng', 'area_detail', 'area_poly', 'res'])

    df_kvt_area_res = df_kvt.merge(df_kvt_area_res, how='left', on=['timestamp_hour', 'id'])
    df_kvt_area_res['area_id'] = df_kvt_area_res['area_id'].fillna(0)
    df_kvt_area_res['area_name'] = df_kvt_area_res['area_name'].fillna('0')

    df_kvt_area_res = df_kvt_area_res.groupby(['timestamp_hour', 'city_id', 'area_id', 'area_name']) \
        .agg({'id': 'count'}) \
        .rename(columns={'id': 'kvt'}) \
        .reset_index()

    # –í—ã–≥—Ä—É–∑–∫–∞ –∑–∞–∫–∞–∑–æ–≤
    select_orders = '''
        SELECT 
            date_trunc('hour', tor."timestamp") AS timestamp_hour ,
            tor.id ,
            tb.city_id ,
            tor.start_lat ,
            tor.start_lng ,
            tor.ride_amount ,
            tor.discount ,
            tor.bike_discount_amount ,
            tor.subscription_price 
        FROM damir.t_orders_revenue tor 
        LEFT JOIN damir.t_bike tb ON tor.bid = tb.id
        WHERE tor."timestamp" >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
    '''

    df_orders = pd.read_sql(select_orders, engine_postgresql)
    # –°–æ–µ–¥–∏–Ω—è—é orders –∏ area
    df_orders_area = df_orders.merge(df_areas, how='cross')
    df_orders_area['res'] = df_orders_area.apply(poly_contains_point_orders, axis=1)
    df_orders_area = df_orders_area[df_orders_area['res'] == True]
    df_orders_area = df_orders_area.drop(
        columns=['city_id', 'start_lat', 'start_lng', 'ride_amount', 'discount', 'bike_discount_amount',
                 'subscription_price', 'area_detail', 'area_poly', 'res'])
    df_orders_areas_res = df_orders.merge(df_orders_area, on=['timestamp_hour', 'id'], how='left')
    df_orders_areas_res['area_id'] = df_orders_areas_res['area_id'].fillna(0)
    df_orders_areas_res['area_name'] = df_orders_areas_res['area_name'].fillna('0')
    df_orders_areas_res = df_orders_areas_res.groupby(['timestamp_hour', 'city_id', 'area_id', 'area_name']) \
        .agg({'id': 'count',
              'ride_amount': 'sum',
              'discount': 'sum',
              'bike_discount_amount': 'sum', 'subscription_price': 'sum'}) \
        .rename(columns={'id': 'poezdok',
                         'ride_amount': 'obzchaya_stoimost',
                         'discount': 'oplacheno_bonusami',
                         'bike_discount_amount': 'skidka',
                         'subscription_price': 'abon'}) \
        .reset_index()
    # –í—ã–≥—Ä—É–∑–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    select_distr = '''
        WITH dolgi AS (
            SELECT
                dolgi.city_id ,
                SUM(dolgi.debit_cash) AS dolgi
            FROM 
            (
                SELECT
                    dolgi.city_id ,
                    tpd.debit_cash
                FROM damir.t_payment_details tpd 
                RIGHT JOIN (
                    SELECT 
                        tbu."date" ,
                        tbu.id,
                        tbu.uid,
                        tbu.bid ,
                        tb.city_id ,
                        tbu.ride_amount 
                    FROM damir.t_bike_use tbu
                    LEFT JOIN t_bike tb ON tbu.bid = tb.id
                    WHERE tbu.ride_status = 2 
                        AND to_timestamp( tbu.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        --AND to_timestamp( tbu.start_time) >= '2025-12-18'::date
                    ) AS dolgi ON tpd.ride_id = dolgi.id 
                ) AS dolgi
            GROUP BY dolgi.city_id
        ),
        vyruchka_s_abonementov AS (
            SELECT 
                distr_poezdki_po_gorodam.city_id,
                sum_uspeh_abon.vyruchka_s_abonementov * distr_poezdki_po_gorodam.coef_goroda AS vyruchka_s_abonementov
            FROM (
                -- –≤—ã—Å—á–∏—Ç—ã–≤–∞—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –ø–æ –ø–æ–µ–∑–¥–∫–∞–º
                SELECT 
                    distr_poezdki_po_gorodam.start_time,
                    distr_poezdki_po_gorodam.city_id,
                    distr_poezdki_po_gorodam.poezdok,
                    -- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ numeric –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–µ –±—ã–ª 0
                    distr_poezdki_po_gorodam.poezdok::numeric / SUM(distr_poezdki_po_gorodam.poezdok) OVER (PARTITION BY distr_poezdki_po_gorodam.start_time) AS coef_goroda
                FROM 
                    (
                    SELECT 
                        TO_CHAR(TO_TIMESTAMP(t_bike_use.start_time), 'YYYY-MM-DD') AS start_time,
                        t_bike.city_id,
                        COUNT(t_bike_use.ride_amount) AS poezdok
                    FROM damir.t_bike_use
                    LEFT JOIN damir.t_bike ON t_bike_use.bid = t_bike.id
                    WHERE t_bike_use.ride_status != 5 
                        AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        --AND TO_TIMESTAMP(t_bike_use.start_time) >= '2025-12-18'::date
                    GROUP BY 1, 2
                    ) 
                    AS distr_poezdki_po_gorodam
            ) AS distr_poezdki_po_gorodam
            LEFT JOIN (
                SELECT 
                    TO_CHAR(t_trade.date, 'YYYY-MM-DD') AS start_time,
                    SUM(COALESCE(t_trade.amount, 0)) AS vyruchka_s_abonementov
                FROM damir.t_trade
                WHERE t_trade.type = 6 
                    AND t_trade.status = 1 
                    AND t_trade.date >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                    --AND t_trade.date >= '2025-12-18'::date
                GROUP BY 1
                ) AS sum_uspeh_abon
                ON distr_poezdki_po_gorodam.start_time = sum_uspeh_abon.start_time
        ),
        sum_mnogor_abon AS (
            SELECT 
                distr_poezdki_po_gorodam.city_id,
                sum_mnogor_abon.sum_mnogor_abon * distr_poezdki_po_gorodam.coef_goroda AS sum_mnogor_abon
            FROM (
                -- –≤—ã—Å—á–∏—Ç—ã–≤–∞—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –ø–æ –ø–æ–µ–∑–¥–∫–∞–º
                SELECT 
                    dp.start_time,
                    dp.city_id,
                    dp.poezdok,
                    dp.poezdok::numeric / NULLIF(SUM(COALESCE(dp.poezdok, 0)) OVER (PARTITION BY dp.start_time), 0) AS coef_goroda
                FROM (
                    SELECT 
                        TO_CHAR(TO_TIMESTAMP(t_bike_use.start_time), 'YYYY-MM-DD') AS start_time,
                        t_bike.city_id,
                        COUNT(t_bike_use.ride_amount) AS poezdok
                    FROM damir.t_bike_use
                    LEFT JOIN damir.t_bike ON t_bike_use.bid = t_bike.id
                    WHERE t_bike_use.ride_status != 5 
                        AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        --AND TO_TIMESTAMP(t_bike_use.start_time) >= '2025-12-18'::date
                    GROUP BY 1, 2
                ) AS dp
            ) AS distr_poezdki_po_gorodam
            LEFT JOIN (
                SELECT 
                    TO_CHAR(t_subscription_mapping.start_time, 'YYYY-MM-DD') AS start_time,
                    SUM(COALESCE(t_subscription.price, 0)) AS sum_mnogor_abon
                FROM damir.t_subscription_mapping
                LEFT JOIN damir.t_subscription ON t_subscription_mapping.subscription_id = t_subscription.id
                WHERE 
                    t_subscription_mapping.start_time >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                    --t_subscription_mapping.start_time >= '2025-12-18'::date 
                GROUP BY 1
            ) AS sum_mnogor_abon
            ON distr_poezdki_po_gorodam.start_time = sum_mnogor_abon.start_time
        )
        SELECT 
            COALESCE(dolgi.city_id , vyruchka_s_abonementov.city_id , sum_mnogor_abon.city_id ) AS city_id,
            COALESCE(dolgi.dolgi, 0) AS dolgi,
            vyruchka_s_abonementov.vyruchka_s_abonementov ,
            sum_mnogor_abon.sum_mnogor_abon
        FROM dolgi
        FULL JOIN vyruchka_s_abonementov ON dolgi.city_id = vyruchka_s_abonementov.city_id 
        FULL JOIN sum_mnogor_abon ON COALESCE(dolgi.city_id , vyruchka_s_abonementov.city_id ) = sum_mnogor_abon.city_id 
    '''
    df_distr = pd.read_sql(select_distr, engine_postgresql)

    # –°–æ–µ–¥–∏–Ω—è—é –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ç–∞–±–ª–∏—Ü–µ–π distr
    df_orders_areas_res = df_orders_areas_res.merge(df_distr, how='left', on='city_id')
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤
    df_orders_areas_res['dolgi'] = pd.to_numeric(df_orders_areas_res['dolgi'], errors='coerce')
    df_orders_areas_res['vyruchka_s_abonementov'] = pd.to_numeric(df_orders_areas_res['vyruchka_s_abonementov'], errors='coerce')
    df_orders_areas_res['sum_mnogor_abon'] = pd.to_numeric(df_orders_areas_res['sum_mnogor_abon'], errors='coerce')
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤
    df_orders_areas_res['dolgi'] = df_orders_areas_res['dolgi'].fillna(0)
    df_orders_areas_res['vyruchka_s_abonementov'] = df_orders_areas_res['vyruchka_s_abonementov'].fillna(0)
    df_orders_areas_res['sum_mnogor_abon'] = df_orders_areas_res['sum_mnogor_abon'].fillna(0)
    df_orders_areas_res = df_orders_areas_res.assign(
        cum_poezdok=df_orders_areas_res.groupby('city_id')['poezdok'].transform('sum'))
    df_orders_areas_res['dolgi_res'] = df_orders_areas_res['dolgi'] * df_orders_areas_res['poezdok'] / \
                                       df_orders_areas_res['cum_poezdok']
    df_orders_areas_res['vyruchka_s_abonementov_res'] = df_orders_areas_res['vyruchka_s_abonementov'] * \
                                                        df_orders_areas_res['poezdok'] / df_orders_areas_res[
                                                            'cum_poezdok']
    df_orders_areas_res['sum_mnogor_abon_res'] = df_orders_areas_res['sum_mnogor_abon'] * df_orders_areas_res[
        'poezdok'] / df_orders_areas_res['cum_poezdok']
    # –°–æ–µ–¥–∏–Ω—è—é –ö–í–¢ –∏ orders
    df_orders_kvt_area_res = df_kvt_area_res.merge(df_orders_areas_res, how='left',
                                                   on=['timestamp_hour', 'city_id', 'area_id', 'area_name'])
    df_orders_kvt_area_res = df_orders_kvt_area_res[['timestamp_hour',
                                                     'city_id',
                                                     'area_id',
                                                     'area_name',
                                                     'kvt',
                                                     'poezdok',
                                                     'obzchaya_stoimost',
                                                     'oplacheno_bonusami',
                                                     'skidka',
                                                     'abon',
                                                     'dolgi_res',
                                                     'vyruchka_s_abonementov_res',
                                                     'sum_mnogor_abon_res']]
    df_orders_kvt_area_res = df_orders_kvt_area_res \
        .rename(columns={'dolgi_res': 'dolgi',
                         'vyruchka_s_abonementov_res': 'vyruchka_s_abonementov',
                         'sum_mnogor_abon_res': 'sum_mnogor_abon'})
    df_orders_kvt_area_res['poezdok'] = df_orders_kvt_area_res['poezdok'].fillna(0).astype(float)
    df_orders_kvt_area_res['obzchaya_stoimost'] = df_orders_kvt_area_res['obzchaya_stoimost'].fillna(0).astype(float)
    df_orders_kvt_area_res['oplacheno_bonusami'] = df_orders_kvt_area_res['oplacheno_bonusami'].fillna(0).astype(float)
    df_orders_kvt_area_res['skidka'] = df_orders_kvt_area_res['skidka'].fillna(0).astype(float)
    df_orders_kvt_area_res['abon'] = df_orders_kvt_area_res['abon'].fillna(0).astype(float)
    df_orders_kvt_area_res['dolgi'] = df_orders_kvt_area_res['dolgi'].fillna(0)
    df_orders_kvt_area_res['vyruchka_s_abonementov'] = df_orders_kvt_area_res['vyruchka_s_abonementov'].fillna(0)
    df_orders_kvt_area_res['sum_mnogor_abon'] = df_orders_kvt_area_res['sum_mnogor_abon'].fillna(0)

    df_orders_kvt_area_res['add_time'] = pd.Timestamp.now()

    truncate_t_area_revenue_stats2 = '''
        DELETE FROM damir.t_area_revenue_stats2
        WHERE damir.t_area_revenue_stats2."timestamp_hour" >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours';
        '''
    with engine_postgresql.connect() as connection:
        with connection.begin() as transaction:
            connection.execute(sa.text(truncate_t_area_revenue_stats2))
            transaction.commit()
            print(f"–¢–∞–±–ª–∏—Ü–∞ t_area_revenue_stats2 —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞!")

    df_orders_kvt_area_res.to_sql("t_area_revenue_stats2", engine_postgresql, if_exists="append", index=False)
    print('–¢–∞–±–ª–∏—Ü–∞ t_area_revenue_stats2 —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!')

    # –í—ã–≥—Ä—É–∑–∫–∞ t_area_revenue_stats2 pandas. –ö–æ–Ω–µ—Ü

    # –í—ã–≥—Ä—É–∑–∫–∞ t_area_revenue_stats3 pandas. –ù–∞—á–∞–ª–æ

    select_kvt = '''
        SELECT 
            res_tab."timestamp" ,
            res_tab.timestamp_hour ,
            res_tab.id ,
            res_tab.city_id ,
            res_tab.g_lat ,
            res_tab.g_lng
        FROM 
        (
            SELECT 
                tbh."timestamp" ,
                date_trunc('hour', tbh."timestamp") AS "timestamp_hour" ,
                tbh.id ,
                tbh.g_lat ,
                tbh.g_lng ,
                tb.city_id ,
                RANK() OVER (PARTITION BY date_trunc('hour', tbh."timestamp") ORDER BY tbh."timestamp" DESC) AS rn
            FROM damir.t_bike_history tbh
            LEFT JOIN damir.t_bike tb ON tbh.id = tb.id 
            WHERE tbh.error_status IN (0,7)
                --AND tbh."timestamp" >= date_trunc('hour', NOW())
                AND tbh."timestamp" >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days'
            ) AS res_tab
        WHERE res_tab.rn = 1
    '''

    df_kvt = pd.read_sql(select_kvt, engine_postgresql)

    # –í—ã–≥—Ä—É–∑–∫–∞ areas
    select_areas = '''
        SELECT
            --ta.city_id ,
            ta.id AS area_id ,
            ta."name" AS area_name ,
            ta.detail AS area_detail
        FROM damir.t_area ta
        WHERE ta."name" LIKE '%%| Area |%%'
    '''

    df_areas = pd.read_sql(select_areas, engine_postgresql)

    # area –≤ –ø–æ–ª–∏–≥–æ–Ω—ã
    df_areas['area_poly'] = df_areas['area_detail'].apply(decode_polyline_to_tuples)
    df_kvt_area = df_kvt.merge(df_areas, how='cross')
    df_kvt_area['res'] = df_kvt_area.apply(poly_contains_point_kvt, axis=1)
    df_kvt_area_res = df_kvt_area[df_kvt_area['res'] == True]
    df_kvt_area_res = df_kvt_area_res.drop(
        columns=['timestamp', 'city_id', 'g_lat', 'g_lng', 'area_detail', 'area_poly', 'res'])
    df_kvt_area_res = df_kvt.merge(df_kvt_area_res, how='left', on=['timestamp_hour', 'id'])
    df_kvt_area_res['area_id'] = df_kvt_area_res['area_id'].fillna(0)
    df_kvt_area_res['area_name'] = df_kvt_area_res['area_name'].fillna('0')

    df_kvt_area_res = df_kvt_area_res.groupby(['timestamp_hour', 'city_id', 'area_id', 'area_name']) \
        .agg({'id': 'count'}) \
        .rename(columns={'id': 'kvt'}) \
        .reset_index()

    # –°–æ–±–∏—Ä–∞—é –≤—Å–µ –≤ –æ–¥–∏–Ω –¥–µ–Ω—å
    df_kvt_area_res['start_day'] = pd.to_datetime(df_kvt_area_res['timestamp_hour'].dt.date)
    df_kvt_area_res = df_kvt_area_res.groupby(['start_day', 'city_id', 'area_id', 'area_name'], as_index=False) \
        .agg({'kvt': 'mean'})

    # –í—ã–≥—Ä—É–∑–∫–∞ –∑–∞–∫–∞–∑–æ–≤
    select_orders = '''
        SELECT
            -- NOW() AS add_time ,
            STR_TO_DATE(DATE_FORMAT(from_unixtime(tbu.`date`), '%Y-%m-%d %H:00:00'), "%Y-%m-%d %H:%i:%s") AS timestamp_hour ,
            -- tbu.`date`,
            from_unixtime(tbu.`date`) AS "timestamp" ,
            tbu.id ,
            tb.city_id ,
            tbu.start_lat ,
            tbu.start_lng ,
            IFNULL(tbu.ride_amount,0) AS ride_amount,
            IFNULL(tbu.discount,0) AS discount,
            IFNULL(tpd.bike_discount_amount,0) AS bike_discount_amount,
            IFNULL(ts.price , 0) AS subscription_price
            -- tbu.bid
        FROM shamri.t_bike_use tbu
        LEFT JOIN shamri.t_payment_details tpd ON tbu.id = tpd.ride_id
        LEFT JOIN shamri.t_subscription ts ON tbu.subscription_id = ts.id 
        LEFT JOIN shamri.t_bike tb ON tb.id = tbu.bid
        WHERE tbu.ride_status != 5
             AND 
            -- from_unixtime(tbu.`date`) >= STR_TO_DATE(DATE_FORMAT(NOW(), '%Y-%m-%d %H:00:00'), "%Y-%m-%d %H:%i:%s") - INTERVAL 2 HOUR
             tbu.`date` >= UNIX_TIMESTAMP(DATE_SUB(CURDATE(), INTERVAL 1 DAY))
                -- AND tbu.`date` <= UNIX_TIMESTAMP(NOW())
        ORDER BY tbu.`date`ASC
    '''

    df_orders = pd.read_sql(select_orders, engine_mysql)

    # –°–æ–µ–¥–∏–Ω—è—é orders –∏ area
    df_orders_area = df_orders.merge(df_areas, how='cross')
    df_orders_area['res'] = df_orders_area.apply(poly_contains_point_orders, axis=1)
    df_orders_area = df_orders_area[df_orders_area['res'] == True]
    df_orders_area = df_orders_area.drop(
        columns=['city_id', 'start_lat', 'start_lng', 'ride_amount', 'discount', 'bike_discount_amount',
                 'subscription_price', 'area_detail', 'area_poly', 'res'])
    df_orders_areas_res = df_orders.merge(df_orders_area, on=['timestamp_hour', 'id'], how='left')
    df_orders_areas_res['area_id'] = df_orders_areas_res['area_id'].fillna(0)
    df_orders_areas_res['area_name'] = df_orders_areas_res['area_name'].fillna('0')
    df_orders_areas_res = df_orders_areas_res.groupby(['timestamp_hour', 'city_id', 'area_id', 'area_name']) \
        .agg({'id': 'count',
              'ride_amount': 'sum',
              'discount': 'sum',
              'bike_discount_amount': 'sum', 'subscription_price': 'sum'}) \
        .rename(columns={'id': 'poezdok',
                         'ride_amount': 'obzchaya_stoimost',
                         'discount': 'oplacheno_bonusami',
                         'bike_discount_amount': 'skidka',
                         'subscription_price': 'abon'}) \
        .reset_index()

    # –°–æ–±–∏—Ä–∞—é –≤—Å–µ –≤ –æ–¥–∏–Ω –¥–µ–Ω—å
    df_orders_areas_res['start_day'] = pd.to_datetime(df_orders_areas_res['timestamp_hour'].dt.date)
    df_orders_areas_res = df_orders_areas_res.groupby(['start_day', 'city_id', 'area_id', 'area_name'], as_index=False) \
        .agg({'poezdok': 'sum',
              'obzchaya_stoimost': 'sum',
              'oplacheno_bonusami': 'sum',
              'skidka': 'sum',
              'abon': 'sum'})

    # –í—ã–≥—Ä—É–∑–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    select_distr = '''
        WITH dolgi AS (
            SELECT 
                dolgi.timestamp_day AS start_time ,
                dolgi.city_id ,
                SUM(dolgi.debit_cash) AS dolgi
            FROM 
            (
                SELECT
                    tpd.created::date AS timestamp_day,
                    dolgi.city_id ,
                    tpd.debit_cash
                FROM damir.t_payment_details tpd 
                RIGHT JOIN (
                    SELECT 
                        tbu."date" ,
                        tbu.id,
                        tbu.uid,
                        tbu.bid ,
                        tb.city_id ,
                        tbu.ride_amount 
                    FROM damir.t_bike_use tbu
                    LEFT JOIN t_bike tb ON tbu.bid = tb.id
                    WHERE tbu.ride_status = 2 
                        --AND to_timestamp( tbu.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        AND to_timestamp( tbu.start_time) >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days'
                    ) AS dolgi ON tpd.ride_id = dolgi.id
                ) AS dolgi 
            GROUP BY dolgi.timestamp_day , dolgi.city_id
        ),
        vyruchka_s_abonementov AS (
            SELECT 
                distr_poezdki_po_gorodam.start_time::date AS start_time,
                distr_poezdki_po_gorodam.city_id,
                sum_uspeh_abon.vyruchka_s_abonementov * distr_poezdki_po_gorodam.coef_goroda AS vyruchka_s_abonementov
            FROM (
                -- –≤—ã—Å—á–∏—Ç—ã–≤–∞—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –ø–æ –ø–æ–µ–∑–¥–∫–∞–º
                SELECT 
                    distr_poezdki_po_gorodam.start_time,
                    distr_poezdki_po_gorodam.city_id,
                    distr_poezdki_po_gorodam.poezdok,
                    -- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ numeric –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–µ –±—ã–ª 0
                    distr_poezdki_po_gorodam.poezdok::numeric / SUM(distr_poezdki_po_gorodam.poezdok) OVER (PARTITION BY distr_poezdki_po_gorodam.start_time) AS coef_goroda
                FROM 
                    (
                    SELECT 
                        TO_CHAR(TO_TIMESTAMP(t_bike_use.start_time), 'YYYY-MM-DD') AS start_time,
                        t_bike.city_id,
                        COUNT(t_bike_use.ride_amount) AS poezdok
                    FROM damir.t_bike_use
                    LEFT JOIN damir.t_bike ON t_bike_use.bid = t_bike.id
                    WHERE t_bike_use.ride_status != 5 
                        --AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days'
                    GROUP BY 1, 2
                    ) AS distr_poezdki_po_gorodam
            ) AS distr_poezdki_po_gorodam
            LEFT JOIN (
                SELECT 
                    TO_CHAR(t_trade.date, 'YYYY-MM-DD') AS start_time,
                    SUM(COALESCE(t_trade.amount, 0)) AS vyruchka_s_abonementov
                FROM damir.t_trade
                WHERE t_trade.type = 6 
                    AND t_trade.status = 1 
                    --AND t_trade.date >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                    AND t_trade.date >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days'
                GROUP BY 1
                ) AS sum_uspeh_abon
                ON distr_poezdki_po_gorodam.start_time = sum_uspeh_abon.start_time
        ),
        sum_mnogor_abon AS (
            SELECT 
                distr_poezdki_po_gorodam.start_time::date AS start_time,
                distr_poezdki_po_gorodam.city_id,
                sum_mnogor_abon.sum_mnogor_abon * distr_poezdki_po_gorodam.coef_goroda AS sum_mnogor_abon
            FROM (
                -- –≤—ã—Å—á–∏—Ç—ã–≤–∞—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –ø–æ –ø–æ–µ–∑–¥–∫–∞–º
                SELECT 
                    dp.start_time,
                    dp.city_id,
                    dp.poezdok,
                    dp.poezdok::numeric / NULLIF(SUM(COALESCE(dp.poezdok, 0)) OVER (PARTITION BY dp.start_time), 0) AS coef_goroda
                FROM (
                    SELECT 
                        TO_CHAR(TO_TIMESTAMP(t_bike_use.start_time), 'YYYY-MM-DD') AS start_time,
                        t_bike.city_id,
                        COUNT(t_bike_use.ride_amount) AS poezdok
                    FROM damir.t_bike_use
                    LEFT JOIN damir.t_bike ON t_bike_use.bid = t_bike.id
                    WHERE t_bike_use.ride_status != 5 
                        --AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                        AND TO_TIMESTAMP(t_bike_use.start_time) >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days'
                    GROUP BY 1, 2
                ) AS dp
            ) AS distr_poezdki_po_gorodam
            LEFT JOIN (
                SELECT 
                    TO_CHAR(t_subscription_mapping.start_time, 'YYYY-MM-DD') AS start_time,
                    SUM(COALESCE(t_subscription.price, 0)) AS sum_mnogor_abon
                FROM damir.t_subscription_mapping
                LEFT JOIN damir.t_subscription ON t_subscription_mapping.subscription_id = t_subscription.id
                WHERE 
                    --t_subscription_mapping.start_time >= date_trunc('hour', NOW() + INTERVAL '2 hours') - INTERVAL '2 hours'
                    t_subscription_mapping.start_time >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days' 
                    GROUP BY 1
                ) AS sum_mnogor_abon
                ON distr_poezdki_po_gorodam.start_time = sum_mnogor_abon.start_time
        )
        SELECT 
            COALESCE(dolgi.start_time , vyruchka_s_abonementov.start_time , sum_mnogor_abon.start_time ) AS start_day ,
            COALESCE(dolgi.city_id , vyruchka_s_abonementov.city_id , sum_mnogor_abon.city_id ) AS city_id,
            COALESCE(dolgi.dolgi, 0) AS dolgi,
            vyruchka_s_abonementov.vyruchka_s_abonementov ,
            sum_mnogor_abon.sum_mnogor_abon
        FROM dolgi
        FULL JOIN vyruchka_s_abonementov ON dolgi.city_id = vyruchka_s_abonementov.city_id AND dolgi.start_time = vyruchka_s_abonementov.start_time
        FULL JOIN sum_mnogor_abon ON COALESCE(dolgi.city_id , vyruchka_s_abonementov.city_id ) = sum_mnogor_abon.city_id AND COALESCE(dolgi.start_time , vyruchka_s_abonementov.start_time ) = sum_mnogor_abon.start_time
    '''

    df_distr = pd.read_sql(select_distr, engine_postgresql)
    df_distr['start_day'] = pd.to_datetime(df_distr['start_day'])

    # –°–æ–µ–¥–∏–Ω—è—é –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ç–∞–±–ª–∏—Ü–µ–π distr
    df_orders_areas_res = df_orders_areas_res.merge(df_distr, how='left', on=['start_day', 'city_id'])
    df_orders_areas_res['dolgi'] = df_orders_areas_res['dolgi'].fillna(0)
    df_orders_areas_res['vyruchka_s_abonementov'] = df_orders_areas_res['vyruchka_s_abonementov'].fillna(0)
    df_orders_areas_res['sum_mnogor_abon'] = df_orders_areas_res['sum_mnogor_abon'].fillna(0)
    df_orders_areas_res = df_orders_areas_res.assign(
        cum_poezdok=df_orders_areas_res.groupby('city_id')['poezdok'].transform('sum'))
    df_orders_areas_res['dolgi_res'] = df_orders_areas_res['dolgi'] * df_orders_areas_res['poezdok'] / \
                                       df_orders_areas_res['cum_poezdok']
    df_orders_areas_res['vyruchka_s_abonementov_res'] = df_orders_areas_res['vyruchka_s_abonementov'] * \
                                                        df_orders_areas_res['poezdok'] / df_orders_areas_res[
                                                            'cum_poezdok']
    df_orders_areas_res['sum_mnogor_abon_res'] = df_orders_areas_res['sum_mnogor_abon'] * df_orders_areas_res[
        'poezdok'] / df_orders_areas_res['cum_poezdok']

    # –°–æ–µ–¥–∏–Ω—è—é –ö–í–¢ –∏ orders
    df_orders_kvt_area_res = df_kvt_area_res.merge(df_orders_areas_res, how='left',
                                                   on=['start_day', 'city_id', 'area_id', 'area_name'])

    df_orders_kvt_area_res = df_orders_kvt_area_res[['start_day',
                                                     'city_id',
                                                     'area_id',
                                                     'area_name',
                                                     'kvt',
                                                     'poezdok',
                                                     'obzchaya_stoimost',
                                                     'oplacheno_bonusami',
                                                     'skidka',
                                                     'abon',
                                                     'dolgi_res',
                                                     'vyruchka_s_abonementov_res',
                                                     'sum_mnogor_abon_res']]
    df_orders_kvt_area_res = df_orders_kvt_area_res \
        .rename(columns={'dolgi_res': 'dolgi',
                         'vyruchka_s_abonementov_res': 'vyruchka_s_abonementov',
                         'sum_mnogor_abon_res': 'sum_mnogor_abon'})

    df_orders_kvt_area_res['poezdok'] = df_orders_kvt_area_res['poezdok'].fillna(0).astype(float)
    df_orders_kvt_area_res['obzchaya_stoimost'] = df_orders_kvt_area_res['obzchaya_stoimost'].fillna(0).astype(float)
    df_orders_kvt_area_res['oplacheno_bonusami'] = df_orders_kvt_area_res['oplacheno_bonusami'].fillna(0).astype(float)
    df_orders_kvt_area_res['skidka'] = df_orders_kvt_area_res['skidka'].fillna(0).astype(float)
    df_orders_kvt_area_res['abon'] = df_orders_kvt_area_res['abon'].fillna(0).astype(float)
    df_orders_kvt_area_res['dolgi'] = df_orders_kvt_area_res['dolgi'].fillna(0)
    df_orders_kvt_area_res['vyruchka_s_abonementov'] = df_orders_kvt_area_res['vyruchka_s_abonementov'].fillna(0)
    df_orders_kvt_area_res['sum_mnogor_abon'] = df_orders_kvt_area_res['sum_mnogor_abon'].fillna(0)
    df_orders_kvt_area_res['add_time'] = pd.Timestamp.now()

    df_orders_kvt_area_res.rename(columns={'start_day': 'timestamp_hour'}, inplace=True)

    truncate_t_area_revenue_stats3 = '''
        DELETE FROM damir.t_area_revenue_stats3
        WHERE damir.t_area_revenue_stats3."timestamp_hour" >= date_trunc('day', (NOW() + INTERVAL '2 hours')) - INTERVAL '1 days';
        '''
    with engine_postgresql.connect() as connection:
        with connection.begin() as transaction:
            connection.execute(sa.text(truncate_t_area_revenue_stats3))
            transaction.commit()
            print(f"–¢–∞–±–ª–∏—Ü–∞ t_area_revenue_stats3 —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞!")


    df_orders_kvt_area_res.to_sql("t_area_revenue_stats3", engine_postgresql, if_exists="append", index=False)
    print('–¢–∞–±–ª–∏—Ü–∞ t_area_revenue_stats3 —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!')

    # –í—ã–≥—Ä—É–∑–∫–∞ t_area_revenue_stats3 pandas. –ö–æ–Ω–µ—Ü

    # –í—ã–≥—Ä—É–∑–∫–∞ t_last_kvt pandas. –ù–∞—á–∞–ª–æ
    # –ö–æ–ø–∏—Ä—É—é t_last_kvt
    select_t_last_kvt = '''
        WITH kvt AS (
            SELECT 
                res.city_id ,
                res.parking_id ,
                res."name" AS parking_name ,
                COUNT(res.id) AS kvt
            FROM 
            (
                SELECT 
                    tb.city_id ,
                    tb.id ,
                    tb.g_lat ,
                    tb.g_lng ,
                    ta.id AS parking_id ,
                    ta."name" ,
                    ta.lat ,
                    ta.lng ,
                    RANK() OVER (PARTITION BY tb.id ORDER BY 6371000 * acos(
                                                                            cos(radians(tb.g_lat)) * 
                                                                            cos(radians(ta.lat)) * 
                                                                            cos(radians(ta.lng) - radians(tb.g_lng)) + 
                                                                            sin(radians(tb.g_lat)) * 
                                                                            sin(radians(ta.lat))) ASC) AS rn
                FROM damir.t_bike tb 
                CROSS JOIN damir.t_area ta
                WHERE tb.error_status IN (0,7)
                    AND ta.active = 1
                ) AS res
            WHERE res.rn = 1
            GROUP BY res.city_id , res.parking_id , res."name"
        ),
        parking_all AS (
            SELECT 
                ta.city_id ,
                ta.id AS parking_id ,
                ta."name" AS parking_name
            FROM damir.t_area ta
            WHERE ta.active = 1
        )
        SELECT
            NOW() AS add_time ,
            COALESCE(kvt.city_id , parking_all.city_id) AS city_id,
            COALESCE(tap.area_id, 0) AS area_id,
            COALESCE(tap.area_name, '0') AS area_name,
            COALESCE(parking_all.parking_id , kvt.parking_id) AS parking_id,
            COALESCE(parking_all.parking_name , kvt.parking_name) AS parking_name,
            COALESCE(kvt.kvt, 0) AS kvt 
        FROM parking_all
        LEFT JOIN kvt ON parking_all.parking_id = kvt.parking_id
        LEFT JOIN damir.t_areas_parkings tap ON COALESCE(parking_all.parking_id , kvt.parking_id) = tap.parking_id
    '''
    df_t_last_kvt = pd.read_sql(select_t_last_kvt, engine_postgresql)

    # –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
    truncate_t_last_kvt = "TRUNCATE TABLE t_last_kvt RESTART IDENTITY;"

    with engine_postgresql.connect() as connection:
        with connection.begin() as transaction:
            print(f"–ü–æ–ø—ã—Ç–∫–∞ –æ—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É")
            # –û—á–∏—Å—Ç–∫–∞ t_bike
            connection.execute(sa.text(truncate_t_last_kvt))
            # –ï—Å–ª–∏ –æ—à–∏–±–æ–∫ –Ω–µ—Ç, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            print(f"–¢–∞–±–ª–∏—Ü–∞ truncate_t_last_kvt —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞!")

    df_t_last_kvt.to_sql("t_last_kvt", engine_postgresql, if_exists="append", index=False)
    print('–¢–∞–±–ª–∏—Ü–∞ t_last_kvt —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!')
    # –í—ã–≥—Ä—É–∑–∫–∞ t_last_kvt pandas. –ö–æ–Ω–µ—Ü

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ Parking metadata –≤ Google Sheets. –ù–∞—á–∞–ª–æ
    SPREADSHEET_ID = '1b1lck8cPfqtBAOuzGjMYra6qnCs2dDn4TeUuB4FWHrU'
    CREDENTIALS_FILE = generated_json_file

    # –ó–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ postgresql
    select_parking_metadata = '''
        SELECT 
            dtprs.parking_id ,
            ROUND(COALESCE(AVG(dtprs.poezdok) FILTER (WHERE (EXTRACT(HOUR FROM dtprs."timestamp") IN (6,7,8,9,10,11,12,13,14,15,16,17)) AND (EXTRACT(DOW FROM dtprs."timestamp") IN (1,2,3,4,5))), 0)) AS target_scooter_count_workday_6_to_18 ,
            ROUND(COALESCE(AVG(dtprs.poezdok) FILTER (WHERE (EXTRACT(HOUR FROM dtprs."timestamp") IN (18,19,20,21,22,23,0,1,2,3,4,5)) AND (EXTRACT(DOW FROM dtprs."timestamp") IN (1,2,3,4,5))), 0)) AS target_scooter_count_workday_18_to_6 ,
            ROUND(COALESCE(AVG(dtprs.poezdok) FILTER (WHERE EXTRACT(DOW FROM dtprs."timestamp") IN (6,7)), 0)) AS target_scooter_count_weekend
        FROM damir.t_parking_revenue_stats1 dtprs
        WHERE dtprs."timestamp"::date >= (NOW() + INTERVAL '2 HOURS')::date - INTERVAL '13 DAY'
        --WHERE dtprs."timestamp"::date >= (NOW())::date - INTERVAL '13 DAY'
        GROUP BY dtprs.parking_id
        ORDER BY dtprs.parking_id ASC
    '''

    df_parking_metadata = pd.read_sql(select_parking_metadata, engine_postgresql)

    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    manager = GoogleSheetsManager(CREDENTIALS_FILE, SPREADSHEET_ID)

    # –£–¥–∞–ª–µ–Ω–∏–µ + –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Google Sheets
    manager.truncate_and_write(
        df=df_parking_metadata,
        range_name='Sheet2!A:D',
        sheet_name='Sheet2',
        use_batch_clear=True,
        include_headers=True
    )

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ Parking metadata –≤ Google Sheets. –ö–æ–Ω–µ—Ü

if __name__ == "__main__":
    main()
